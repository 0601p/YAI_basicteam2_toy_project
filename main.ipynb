{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/misrakahmed/vegetable-image-dataset\n",
    "\n",
    "데이터를 분류하는 데에 여러 모델들을 사용해 보고 성능, 학습 시간 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "# 필수 코드\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import visdom\n",
    "\n",
    "vis = visdom.Visdom()\n",
    "vis.close(env='main')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 256\n",
    "learning_rate = 1e-4\n",
    "epoch = 15\n",
    "\n",
    "image_size = 56\n",
    "class_num = 15\n",
    "class_name = ['Bean', 'Bitter_Gourd', 'Bottle_Gourd', 'Brinjal', 'Broccoli', 'Cabbage', 'Capsicum', 'Carrot', 'Cauliflower', 'Cucumber', 'Papaya', 'Potato', 'Pumpkin', 'Radish', 'Tomato']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data loader를 반환함\n",
    "def get_train_data_loaders(resize = False, rotate = False):\n",
    "    transforms = []\n",
    "\n",
    "    if rotate:\n",
    "        transforms.append(torchvision.transforms.transforms.RandomRotation(360))\n",
    "\n",
    "    if resize:\n",
    "        transforms.append(torchvision.transforms.RandomResizedCrop((image_size, image_size)))\n",
    "        \n",
    "    transforms.append(torchvision.transforms.Resize((image_size, image_size)))\n",
    "    transforms.append(torchvision.transforms.ToTensor())\n",
    "    transform = torchvision.transforms.Compose(transforms)\n",
    "\n",
    "    train_data = torchvision.datasets.ImageFolder(root='train', transform=transform)\n",
    "    train_data_loader =torch.utils.data.DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    return train_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data loader와 validation data loader를 반환함\n",
    "def get_test_validation_data_loaders():\n",
    "    transform = torchvision.transforms.Compose([torchvision.transforms.Resize((image_size, image_size)), torchvision.transforms.ToTensor()])\n",
    "    test_data = torchvision.datasets.ImageFolder(root='test', transform=transform)\n",
    "    validation_data = torchvision.datasets.ImageFolder(root='validation', transform=transform)\n",
    "\n",
    "    test_data_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    validation_data_loader =torch.utils.data.DataLoader(dataset = validation_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    return test_data_loader, validation_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, data_loader):\n",
    "    with torch.no_grad():\n",
    "        accuracy = 0\n",
    "        for X, Y in data_loader:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            prediction = model(X)\n",
    "            correct_prediction = torch.argmax(prediction, dim=1) == Y\n",
    "            accuracy += correct_prediction.float().mean()\n",
    "        accuracy /= len(data_loader)\n",
    "        print(\"accuracy : {:.4f}%\".format(accuracy.item() * 100))\n",
    "    return accuracy.item() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model을 data_loader로 epoch만큼 학습함, print_loss가 True이면 trainloss를 출력함\n",
    "def train_model(model, data_loader, test_loader, epochs, print_loss=False):\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    total_batch = len(data_loader)\n",
    "\n",
    "    if print_loss:\n",
    "        loss_plot = vis.line(Y=torch.Tensor(1).zero_(),opts=dict(title='loss_tracker', legend=['loss'], showlegend=True), env='main')\n",
    "        acc_plot = vis.line(Y=torch.Tensor(1).zero_(),opts=dict(title='accuracy_tracker', legend=['accuracy'], showlegend=True), env='main')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0.0\n",
    "        for X, Y in data_loader:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_cost += loss/total_batch\n",
    "        \n",
    "        del X\n",
    "        del Y\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if print_loss:\n",
    "            print('[EPOCH:{:3d}] cost:{:.5f}'.format(epoch + 1, avg_cost))\n",
    "            vis.line(X=torch.Tensor([epoch]), Y=torch.Tensor([avg_cost]), win=loss_plot, update=\"append\")\n",
    "            acc = test_model(model, test_loader)\n",
    "            vis.line(X=torch.Tensor([epoch]), Y=torch.Tensor([acc]), win=acc_plot, update=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected model\n",
    "class FCmodel(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        layer_list = []\n",
    "        in_size = config[0]\n",
    "        for i in range(1, len(config) - 1):\n",
    "            out_size = config[i]\n",
    "            layer_list.append(torch.nn.Linear(in_size, out_size, bias=True))\n",
    "            layer_list.append(torch.nn.ReLU())\n",
    "            in_size = out_size\n",
    "        \n",
    "        out_size = config[-1]\n",
    "        layer_list.append(torch.nn.Linear(in_size, out_size, bias=True))\n",
    "\n",
    "        self.layers = torch.nn.Sequential(*layer_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, config, config_fc = None):\n",
    "        super().__init__()\n",
    "        layer_list = []\n",
    "        channel_in_size = 3\n",
    "        in_size = image_size\n",
    "        for cont in config:\n",
    "            if cont == 'M':\n",
    "                layer_list.append(torch.nn.MaxPool2d(2))\n",
    "                in_size = in_size // 2\n",
    "            else:\n",
    "                channel, kernel_size, padding = cont\n",
    "                layer_list.append(torch.nn.Conv2d(channel_in_size, channel, kernel_size, padding=padding))\n",
    "                layer_list.append(torch.nn.ReLU())\n",
    "                channel_in_size = channel\n",
    "                in_size = in_size - kernel_size + 2 * padding + 1\n",
    "        \n",
    "        self.layers = torch.nn.Sequential(*layer_list)\n",
    "\n",
    "        if config_fc == None:\n",
    "            self.fc = FCmodel([in_size * in_size * channel_in_size, 128, 32, class_num])\n",
    "        else:\n",
    "            self.fc = FCmodel(config_fc)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG model\n",
    "class VGG(torch.nn.Module):\n",
    "    def __init__(self, config, init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        \n",
    "        channel_in_size = 3\n",
    "        layer_list = []\n",
    "        for cont in config:\n",
    "            if cont == 'M':\n",
    "                layer_list.append(torch.nn.MaxPool2d(2))\n",
    "            else:\n",
    "                layer_list.append(torch.nn.Conv2d(channel_in_size, cont, 3, padding=1))\n",
    "                layer_list.append(torch.nn.ReLU())\n",
    "                channel_in_size = cont\n",
    "        \n",
    "        self.features = torch.nn.Sequential(*layer_list)\n",
    "\n",
    "        \n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d((7, 7))\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(channel_in_size * 7 * 7, 512),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(512, 128),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(128, class_num),\n",
    "        )#FC layer\n",
    "        \n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x) #Convolution \n",
    "        x = self.avgpool(x) # avgpool\n",
    "        x = x.view(x.size(0), -1) #\n",
    "        x = self.classifier(x) #FC layer\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.BatchNorm2d):\n",
    "                torch.nn.init.constant_(m.weight, 1)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight, 0, 0.01)\n",
    "                torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionA(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionA, self).__init__()\n",
    "        \n",
    "        self.branch_pool = torch.nn.Conv2d(in_channels, 24, kernel_size=1)\n",
    "        \n",
    "        self.branch1x1 = torch.nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        \n",
    "        self.branch5x5_1 = torch.nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch5x5_2 = torch.nn.Conv2d(16,24,kernel_size=5, padding=2)\n",
    "        \n",
    "        self.branch3x3dbl_1 = torch.nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = torch.nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = torch.nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        \n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "        \n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "        \n",
    "        branch_pool = torch.nn.functional.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "        \n",
    "        outputs = [branch1x1,  branch5x5, branch3x3dbl, branch_pool]\n",
    "        \n",
    "        return torch.cat(outputs, 1)\n",
    "    \n",
    "class InceptionNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InceptionNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3,10,kernel_size=5)\n",
    "        self.conv2 = torch.nn.Conv2d(88,20,kernel_size=5)\n",
    "        \n",
    "        self.incept1 = InceptionA(in_channels=10)\n",
    "        self.incept2 = InceptionA(in_channels=20)\n",
    "        \n",
    "        self.mp = torch.nn.MaxPool2d(2)\n",
    "        self.fc = torch.nn.Linear(1408,15)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = torch.relu(self.mp(self.conv1(x)))\n",
    "        x = self.incept1(x)\n",
    "        x = torch.relu(self.mp(self.conv2(x)))\n",
    "        x = self.incept2(x)\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return torch.nn.functional.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InceptionNet(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(88, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (incept1): InceptionA(\n",
      "    (branch_pool): Conv2d(10, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (branch1x1): Conv2d(10, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (branch5x5_1): Conv2d(10, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (branch5x5_2): Conv2d(16, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (branch3x3dbl_1): Conv2d(10, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (branch3x3dbl_2): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (branch3x3dbl_3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (incept2): InceptionA(\n",
      "    (branch_pool): Conv2d(20, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (branch1x1): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (branch5x5_1): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (branch5x5_2): Conv2d(16, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (branch3x3dbl_1): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (branch3x3dbl_2): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (branch3x3dbl_3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Linear(in_features=1408, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#model1 = FCmodel([image_size*image_size*3, 2048, 512, 128, class_num]).to(device)\n",
    "#model2 = CNN([(32, 3, 0), 'M', (64, 3, 0), 'M', (128, 3, 0), 'M']).to(device)\n",
    "#model3 = VGG([64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512]).to(device)\n",
    "model4 = InceptionNet().to(device)\n",
    "print(model4)\n",
    "train_loader = get_train_data_loaders()\n",
    "test_loader, valid_loader = get_test_validation_data_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (256x10648 and 1408x15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11176\\2261096870.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11176\\3056566146.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, data_loader, test_loader, epochs, print_loss)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11176\\1453727955.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mincept2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (256x10648 and 1408x15)"
     ]
    }
   ],
   "source": [
    "train_model(model4, train_loader, test_loader, epoch, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16732\\1757765691.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_test_validation_data_loaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model2' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9fbae1f717372853f059f30b1665e3de40574a1ddd27eaeebd4164e889f9321"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
