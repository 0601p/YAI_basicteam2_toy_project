{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/misrakahmed/vegetable-image-dataset\n",
    "\n",
    "데이터를 분류하는 데에 여러 모델들을 사용해 보고 성능, 학습 시간 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필수 코드\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "#import visdom\n",
    "\n",
    "#vis = visdom.Visdom()\n",
    "#vis.close(env='main')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "epoch = 100\n",
    "\n",
    "image_size = 56\n",
    "class_num = 15\n",
    "class_name = ['Bean', 'Bitter_Gourd', 'Bottle_Gourd', 'Brinjal', 'Broccoli', 'Cabbage', 'Capsicum', 'Carrot', 'Cauliflower', 'Cucumber', 'Papaya', 'Potato', 'Pumpkin', 'Radish', 'Tomato']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data loader를 반환함\n",
    "def get_train_data_loaders(resize = False, rotate = False):\n",
    "    transforms = []\n",
    "\n",
    "    if rotate:\n",
    "        transforms.append(torchvision.transforms.transforms.RandomRotation(360))\n",
    "\n",
    "    if resize:\n",
    "        transforms.append(torchvision.transforms.RandomResizedCrop((image_size, image_size)))\n",
    "        \n",
    "    transforms.append(torchvision.transforms.Resize((image_size, image_size)))\n",
    "    transforms.append(torchvision.transforms.ToTensor())\n",
    "    transform = torchvision.transforms.Compose(transforms)\n",
    "\n",
    "    train_data = torchvision.datasets.ImageFolder(root='train', transform=transform)\n",
    "    train_data_loader =torch.utils.data.DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    return train_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data loader와 validation data loader를 반환함\n",
    "def get_test_validation_data_loaders():\n",
    "    transform = torchvision.transforms.Compose([torchvision.transforms.Resize((image_size, image_size)), torchvision.transforms.ToTensor()])\n",
    "    test_data = torchvision.datasets.ImageFolder(root='test', transform=transform)\n",
    "    validation_data = torchvision.datasets.ImageFolder(root='validation', transform=transform)\n",
    "\n",
    "    test_data_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    validation_data_loader =torch.utils.data.DataLoader(dataset = validation_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    return test_data_loader, validation_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, data_loader):\n",
    "    with torch.no_grad():\n",
    "        accuracy = 0\n",
    "        for X, Y in data_loader:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            prediction = model(X)\n",
    "            correct_prediction = torch.argmax(prediction, dim=1) == Y\n",
    "            accuracy += correct_prediction.float().mean()\n",
    "        accuracy /= len(data_loader)\n",
    "\n",
    "    return accuracy.item() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model을 data_loader로 epoch만큼 학습함, print_loss가 True이면 trainloss를 출력함\n",
    "def train_model(model, data_loader, test_loader, opt, epochs, print_loss=False):\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = opt(model.parameters(), lr=learning_rate)\n",
    "    total_batch = len(data_loader)\n",
    "\n",
    "    if print_loss:\n",
    "        #loss_plot = vis.line(Y=torch.Tensor(1).zero_(),opts=dict(title='loss_tracker', legend=['loss'], showlegend=True), env='main')\n",
    "        #acc_plot = vis.line(Y=torch.Tensor(1).zero_(),opts=dict(title='accuracy_tracker', legend=['accuracy'], showlegend=True), env='main')\n",
    "        pass\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0.0\n",
    "        for X, Y in data_loader:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_cost += loss/total_batch\n",
    "\n",
    "        if print_loss:\n",
    "            acc = test_model(model, test_loader)\n",
    "            print('[EPOCH:{:3d}] cost:{:.5f} accuracy:{:.5f}'.format(epoch + 1, avg_cost, acc))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected model\n",
    "class FCmodel(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        layer_list = []\n",
    "        in_size = config[0]\n",
    "        for i in range(1, len(config) - 1):\n",
    "            out_size = config[i]\n",
    "            layer_list.append(torch.nn.Linear(in_size, out_size, bias=True))\n",
    "            layer_list.append(torch.nn.ReLU())\n",
    "            in_size = out_size\n",
    "        \n",
    "        out_size = config[-1]\n",
    "        layer_list.append(torch.nn.Linear(in_size, out_size, bias=True))\n",
    "\n",
    "        self.layers = torch.nn.Sequential(*layer_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, config, config_fc = None):\n",
    "        super().__init__()\n",
    "        layer_list = []\n",
    "        channel_in_size = 3\n",
    "        in_size = image_size\n",
    "        for cont in config:\n",
    "            if cont == 'M':\n",
    "                layer_list.append(torch.nn.MaxPool2d(2))\n",
    "                in_size = in_size // 2\n",
    "            else:\n",
    "                channel, kernel_size, padding = cont\n",
    "                layer_list.append(torch.nn.Conv2d(channel_in_size, channel, kernel_size, padding=padding))\n",
    "                layer_list.append(torch.nn.ReLU())\n",
    "                channel_in_size = channel\n",
    "                in_size = in_size - kernel_size + 2 * padding + 1\n",
    "        \n",
    "        self.layers = torch.nn.Sequential(*layer_list)\n",
    "\n",
    "        if config_fc == None:\n",
    "            self.fc = FCmodel([in_size * in_size * channel_in_size, 128, 32, class_num])\n",
    "        else:\n",
    "            self.fc = FCmodel(config_fc)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG model\n",
    "class VGG(torch.nn.Module):\n",
    "    def __init__(self, config, init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        \n",
    "        channel_in_size = 3\n",
    "        layer_list = []\n",
    "        for cont in config:\n",
    "            if cont == 'M':\n",
    "                layer_list.append(torch.nn.MaxPool2d(2))\n",
    "            else:\n",
    "                layer_list.append(torch.nn.Conv2d(channel_in_size, cont, 3, padding=1))\n",
    "                layer_list.append(torch.nn.ReLU())\n",
    "                channel_in_size = cont\n",
    "        \n",
    "        self.features = torch.nn.Sequential(*layer_list)\n",
    "\n",
    "        \n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d((7, 7))\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(channel_in_size * 7 * 7, 512),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(512, 128),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(128, class_num),\n",
    "        )#FC layer\n",
    "        \n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x) #Convolution \n",
    "        x = self.avgpool(x) # avgpool\n",
    "        x = x.view(x.size(0), -1) #\n",
    "        x = self.classifier(x) #FC layer\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.BatchNorm2d):\n",
    "                torch.nn.init.constant_(m.weight, 1)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight, 0, 0.01)\n",
    "                torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionA(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionA, self).__init__()\n",
    "        \n",
    "        self.branch_pool = torch.nn.Conv2d(in_channels, 24, kernel_size=1)\n",
    "        \n",
    "        self.branch1x1 = torch.nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        \n",
    "        self.branch5x5_1 = torch.nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch5x5_2 = torch.nn.Conv2d(16,24,kernel_size=5, padding=2)\n",
    "        \n",
    "        self.branch3x3dbl_1 = torch.nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = torch.nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = torch.nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        \n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "        \n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "        \n",
    "        branch_pool = torch.nn.functional.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "        \n",
    "        outputs = [branch1x1,  branch5x5, branch3x3dbl, branch_pool]\n",
    "        \n",
    "        return torch.cat(outputs, 1)\n",
    "    \n",
    "class InceptionNet(torch.nn.Module):\n",
    "    def __init__(self, activation = 'ReLU'):\n",
    "        super(InceptionNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3,10,kernel_size=5)\n",
    "        self.conv2 = torch.nn.Conv2d(88,20,kernel_size=5)\n",
    "        self.conv3 = torch.nn.Conv2d(88,30,kernel_size=5)\n",
    "        \n",
    "        self.incept1 = InceptionA(in_channels=10)\n",
    "        self.incept2 = InceptionA(in_channels=20)\n",
    "        self.incept3 = InceptionA(in_channels=30)\n",
    "        af = {'ReLU' : torch.nn.ReLU(), 'Sigmoid' : torch.nn.Sigmoid(), 'LeakyReLU' : torch.nn.LeakyReLU(), 'tanh' : torch.nn.Tanh()}\n",
    "        self.activation = af[activation]\n",
    "\n",
    "        self.mp = torch.nn.MaxPool2d(2)\n",
    "        self.fc = torch.nn.Linear(792,class_num)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = self.activation(self.mp(self.conv1(x)))\n",
    "        x = self.incept1(x)\n",
    "        x = self.activation(self.mp(self.conv2(x)))\n",
    "        x = self.incept2(x)\n",
    "        x = self.activation(self.mp(self.conv3(x)))\n",
    "        x = self.incept3(x)\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1 = FCmodel([image_size*image_size*3, 2048, 512, 128, class_num]).to(device)\n",
    "#model2 = CNN([(32, 3, 0), 'M', (64, 3, 0), 'M', (128, 3, 0), 'M']).to(device)\n",
    "#model3 = VGG([64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512]).to(device)\n",
    "# model4 = InceptionNet('ReLU').to(device)\n",
    "# print(model4)\n",
    "train_loader = get_train_data_loaders()\n",
    "test_loader, valid_loader = get_test_validation_data_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH:  1] cost:1.85651 accuracy:51.73234\n",
      "[EPOCH:  2] cost:1.13741 accuracy:65.79484\n",
      "[EPOCH:  3] cost:0.83639 accuracy:77.07201\n",
      "[EPOCH:  4] cost:0.66969 accuracy:75.50951\n",
      "[EPOCH:  5] cost:0.54510 accuracy:82.88044\n",
      "[EPOCH:  6] cost:0.46177 accuracy:84.20516\n",
      "[EPOCH:  7] cost:0.41931 accuracy:85.02038\n",
      "[EPOCH:  8] cost:0.35917 accuracy:84.17120\n",
      "[EPOCH:  9] cost:0.30310 accuracy:88.21332\n",
      "[EPOCH: 10] cost:0.29150 accuracy:89.84375\n",
      "[EPOCH: 11] cost:0.26261 accuracy:90.31929\n",
      "[EPOCH: 12] cost:0.24340 accuracy:89.33424\n",
      "[EPOCH: 13] cost:0.21033 accuracy:89.40217\n",
      "[EPOCH: 14] cost:0.20690 accuracy:89.53804\n",
      "[EPOCH: 15] cost:0.19650 accuracy:90.79484\n",
      "[EPOCH: 16] cost:0.19741 accuracy:86.48098\n",
      "[EPOCH: 17] cost:0.17957 accuracy:90.82881\n",
      "[EPOCH: 18] cost:0.15757 accuracy:92.15354\n",
      "[EPOCH: 19] cost:0.15623 accuracy:90.35326\n",
      "[EPOCH: 20] cost:0.15512 accuracy:90.65897\n",
      "[EPOCH: 21] cost:0.12819 accuracy:92.83288\n",
      "[EPOCH: 22] cost:0.11662 accuracy:91.16848\n",
      "[EPOCH: 23] cost:0.13425 accuracy:90.52310\n",
      "[EPOCH: 24] cost:0.14847 accuracy:91.06658\n",
      "[EPOCH: 25] cost:0.10702 accuracy:90.76087\n",
      "[EPOCH: 26] cost:0.10778 accuracy:90.99864\n",
      "[EPOCH: 27] cost:0.12810 accuracy:88.17935\n",
      "[EPOCH: 28] cost:0.09681 accuracy:91.47419\n",
      "[EPOCH: 29] cost:0.14261 accuracy:92.25544\n",
      "[EPOCH: 30] cost:0.08937 accuracy:91.91576\n",
      "[EPOCH: 31] cost:0.08920 accuracy:92.22147\n",
      "[EPOCH: 32] cost:0.09798 accuracy:93.91984\n",
      "[EPOCH: 33] cost:0.06790 accuracy:92.11957\n",
      "[EPOCH: 34] cost:0.11447 accuracy:92.35734\n",
      "[EPOCH: 35] cost:0.09285 accuracy:92.76495\n",
      "[EPOCH: 36] cost:0.08405 accuracy:94.25951\n",
      "[EPOCH: 37] cost:0.07206 accuracy:91.16848\n",
      "[EPOCH: 38] cost:0.10061 accuracy:91.20245\n",
      "[EPOCH: 39] cost:0.11940 accuracy:92.18750\n",
      "[EPOCH: 40] cost:0.05258 accuracy:94.19158\n",
      "[EPOCH: 41] cost:0.04732 accuracy:93.41033\n",
      "[EPOCH: 42] cost:0.13632 accuracy:90.72691\n",
      "[EPOCH: 43] cost:0.08781 accuracy:93.88587\n",
      "[EPOCH: 44] cost:0.06264 accuracy:93.68207\n",
      "[EPOCH: 45] cost:0.08093 accuracy:92.73098\n",
      "[EPOCH: 46] cost:0.10662 accuracy:90.99864\n",
      "[EPOCH: 47] cost:0.07073 accuracy:92.39131\n",
      "[EPOCH: 48] cost:0.05511 accuracy:93.47826\n",
      "[EPOCH: 49] cost:0.12447 accuracy:88.11141\n",
      "[EPOCH: 50] cost:0.06987 accuracy:94.63316\n",
      "[EPOCH: 51] cost:0.02430 accuracy:94.90489\n",
      "[EPOCH: 52] cost:0.06221 accuracy:91.98370\n",
      "[EPOCH: 53] cost:0.10654 accuracy:92.22147\n",
      "[EPOCH: 54] cost:0.08135 accuracy:92.01766\n",
      "[EPOCH: 55] cost:0.04879 accuracy:92.93479\n",
      "[EPOCH: 56] cost:0.06010 accuracy:92.18750\n",
      "[EPOCH: 57] cost:0.09467 accuracy:93.13859\n",
      "[EPOCH: 58] cost:0.11827 accuracy:93.24049\n",
      "[EPOCH: 59] cost:0.05762 accuracy:94.56522\n",
      "[EPOCH: 60] cost:0.03854 accuracy:93.88587\n",
      "[EPOCH: 61] cost:0.02792 accuracy:94.53125\n",
      "[EPOCH: 62] cost:0.02672 accuracy:92.45924\n",
      "[EPOCH: 63] cost:0.10547 accuracy:93.44429\n",
      "[EPOCH: 64] cost:0.08042 accuracy:93.78397\n",
      "[EPOCH: 65] cost:0.05565 accuracy:92.83288\n",
      "[EPOCH: 66] cost:0.06060 accuracy:93.17256\n",
      "[EPOCH: 67] cost:0.06841 accuracy:92.86685\n",
      "[EPOCH: 68] cost:0.07956 accuracy:92.11957\n",
      "[EPOCH: 69] cost:0.05444 accuracy:94.87092\n",
      "[EPOCH: 70] cost:0.04652 accuracy:94.39538\n",
      "[EPOCH: 71] cost:0.07569 accuracy:92.59511\n",
      "[EPOCH: 72] cost:0.07109 accuracy:91.50816\n",
      "[EPOCH: 73] cost:0.07669 accuracy:93.51223\n",
      "[EPOCH: 74] cost:0.03924 accuracy:92.01766\n",
      "[EPOCH: 75] cost:0.02654 accuracy:93.91984\n",
      "[EPOCH: 76] cost:0.02795 accuracy:93.03669\n",
      "[EPOCH: 77] cost:0.10106 accuracy:93.07066\n",
      "[EPOCH: 78] cost:0.05681 accuracy:93.88587\n",
      "[EPOCH: 79] cost:0.06283 accuracy:92.56114\n",
      "[EPOCH: 80] cost:0.06126 accuracy:92.25544\n",
      "[EPOCH: 81] cost:0.07014 accuracy:90.38723\n",
      "[EPOCH: 82] cost:0.05468 accuracy:94.05571\n",
      "[EPOCH: 83] cost:0.03497 accuracy:91.54212\n",
      "[EPOCH: 84] cost:0.08426 accuracy:93.07066\n",
      "[EPOCH: 85] cost:0.05418 accuracy:93.24049\n",
      "[EPOCH: 86] cost:0.07675 accuracy:94.42935\n",
      "[EPOCH: 87] cost:0.03692 accuracy:94.66712\n",
      "[EPOCH: 88] cost:0.00583 accuracy:95.27854\n",
      "[EPOCH: 89] cost:0.00968 accuracy:94.63316\n",
      "[EPOCH: 90] cost:0.11573 accuracy:88.58696\n",
      "[EPOCH: 91] cost:0.06373 accuracy:92.66304\n",
      "[EPOCH: 92] cost:0.05665 accuracy:94.29348\n",
      "[EPOCH: 93] cost:0.05524 accuracy:93.88587\n",
      "[EPOCH: 94] cost:0.05794 accuracy:92.69701\n",
      "[EPOCH: 95] cost:0.05176 accuracy:94.19158\n",
      "[EPOCH: 96] cost:0.03550 accuracy:94.15761\n",
      "[EPOCH: 97] cost:0.04010 accuracy:93.47826\n",
      "[EPOCH: 98] cost:0.07541 accuracy:94.25951\n",
      "[EPOCH: 99] cost:0.06199 accuracy:93.75000\n",
      "[EPOCH:100] cost:0.02817 accuracy:94.29348\n"
     ]
    }
   ],
   "source": [
    "model4 = InceptionNet('ReLU').to(device)\n",
    "#print(model4)\n",
    "train_model(model4, train_loader, test_loader, torch.optim.Adam, epoch, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size가 생각보다 학습에 많은 영향을 줌! 너무 큰 Batch는 오히려 성능을 저하시키는 것 같음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "020eb92898ac559bd98722b535e6235e035f297f752a3a61b26b02297b428980"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
